# -*- coding: utf-8 -*-
"""Udemy_sp_deeplearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OBIUrVet5stZQ3oB3SjWxVxqMxXgMcnE
"""

import yfinance as yf
import numpy as np
import pandas as pd
import tensorflow as tf

data = yf.download('GOOGL', start="2018-01-01", interval='1d')

data.head()

data.sort_index(inplace=True)

data.isnull().sum()

data.describe()

import plotly.graph_objects as go
fig = go.Figure()
fig.add_trace(go.Scatter(x=data.index, y=data['Close'], mode='lines'))
fig.update_layout(height=500, width=900, xaxis_title="Price", yaxis_title="time")
fig.show()

from sklearn.preprocessing import MinMaxScaler

import pickle
from tqdm.notebook import tnrange

data = data[['Close','Volume']]

test_length = data[(data.index >= '2021-01-01')].shape[0]

def creates_features_and_targets(data, feature_length):
  X = []
  Y = []
  for i in tnrange(len(data) - feature_length):
    X.append(data.iloc[i:i+feature_length,:].values)
    Y.append(data['Close'].values[i+feature_length])

  X = np.array(X)
  Y = np.array(Y)
  return X,Y

X,Y = creates_features_and_targets(data,32)

X.shape, Y.shape

xtrain, xtest, ytrain, ytest = X[:-test_length], X[-test_length:], Y[:-test_length], Y[-test_length:]

class MultiDimensionScalar():
  def __init__(self):
    self.scalers = []

  def fit_transform(self,x):
    total_dims = x.shape[2]
    for i in range(total_dims):
      scaler = MinMaxScaler()
      x[:,:,i] = scaler.fit_transform(x[:,:,i])
      self.scalers.append(scaler)
    return x
  def transform(self,x):
    for i in range(x.shape[2]):
      x[:,:,i] = self.scalers[i].transform(x[:,:,i])
    return x

Feature_Scaler = MultiDimensionScalar()
xtrain = Feature_Scaler.fit_transform(xtrain)
xtest = Feature_Scaler.transform(xtest)

target_Scaler = MinMaxScaler()
ytrain = target_Scaler.fit_transform(ytrain.reshape(-1,1))
ytest = target_Scaler.transform(ytest.reshape(-1,1))

def save_object(obj, name, str):
  pickle_out = open(f"{name}.pck","wb")
  pickle.dump(obj,pickle_out)
  pickle_out.close()

def load_object(name:str):
  pickle_in = open(f"{name}.pck","rb")
  data = pickle.load(pickle_in)
  return data

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
save_best = ModelCheckpoint('best_weights.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25,patience=5,min_lr=0.00001,verbose=1)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Bidirectional, LSTM

model = Sequential()
model.add(Bidirectional(LSTM(512,return_sequences=True,recurrent_dropout=0.1,input_shape = (32,2))))
model.add(LSTM(256, recurrent_dropout = 0.1))
model.add(Dropout(0.3))
model.add(Dense(64, activation='elu'))
model.add(Dropout(0.3))
model.add(Dense(32,activation='elu'))
model.add(Dense(1, activation='linear'))

optimizer = tf.keras.optimizers.SGD(learning_rate = 0.002)
model.compile(loss='mse', optimizer=optimizer)

history = model.fit(xtrain, ytrain, epochs = 10, verbose=1, batch_size=1, shuffle=False, validation_data=(xtest, ytest),callbacks=[reduce_lr, save_best])

